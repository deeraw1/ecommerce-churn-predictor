# app.py
from fastapi import FastAPI, HTTPException, UploadFile, File, Query
import joblib
import numpy as np
import pandas as pd
from sklearn.base import BaseEstimator, TransformerMixin
from pydantic import BaseModel
import traceback
import json
from typing import List, Optional
import io
from datetime import datetime
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse, JSONResponse, HTMLResponse, Response
import os
import uvicorn
from huggingface_hub import InferenceClient
from dotenv import load_dotenv
from supabase import create_client, Client

# ----------------- ENV + CLIENT SETUP -----------------
load_dotenv(".env")

# HuggingFace
hf_api_key = os.getenv("HF_API_KEY")
client = InferenceClient(api_key=hf_api_key) if hf_api_key else None

# Supabase
SUPABASE_URL = os.getenv("SUPABASE_URL", "https://ngpxwvzhpupbopddmhlg.supabase.co")
SUPABASE_SERVICE_KEY = os.getenv("SUPABASE_SERVICE_KEY")  # must be set in .env
if not SUPABASE_SERVICE_KEY:
    raise RuntimeError("SUPABASE_SERVICE_KEY missing. Put service role key in .env as SUPABASE_SERVICE_KEY")

supabase: Client = create_client(SUPABASE_URL, SUPABASE_SERVICE_KEY)

# NOTE: Run the SQL migration in Supabase SQL editor (provided earlier) so the 'predictions' table exists.
# SQL (run once in Supabase SQL editor):
# ------------------------------------------------------
# create table if not exists predictions (
#     id bigint generated by default as identity primary key,
#     tenure int8,
#     numberofaddress int8,
#     cashbackamount float8,
#     daysincelastorder int8,
#     ordercount int8,
#     satisfactionscore int8,
#     churn_prediction int8,
#     churn_probability float8,
#     timestamp timestamptz default now(),
#     input_data jsonb,
#     insights jsonb
# );
# create index if not exists idx_timestamp on predictions (timestamp desc);
# create index if not exists idx_churn on predictions (churn_prediction);
# create index if not exists idx_probability on predictions (churn_probability);
# ------------------------------------------------------

# ==================== PYDANTIC MODELS ====================
class CustomerData(BaseModel):
    tenure: int
    numberofaddress: int
    cashbackamount: float
    daysincelastorder: int
    ordercount: int
    satisfactionscore: int

class PredictionResponse(BaseModel):
    churn_prediction: int
    churn_probability: float
    threshold_used: float
    risk_level: str
    insights: List[str]
    message: str

# ==================== DATA SANITIZER CLASS ====================
class DataSanitizer(BaseEstimator, TransformerMixin):
    def __init__(self, feature_config):
        self.feature_config = feature_config

    def fit(self, X, y=None):
        self.feature_config['numeric_means'] = {col: X[col].mean() for col in self.feature_config['numeric_features']}
        return self

    def transform(self, X):
        X = X.rename(columns=lambda x: x.strip().lower())
        expected = [col for col in self.feature_config['expected_features']]
        missing = [f for f in expected if f not in X.columns]
        for col in missing:
            X[col] = self.feature_config['numeric_means'][col]
        X = X[expected]
        for col in expected:
            X[col] = pd.to_numeric(X[col], errors='coerce').fillna(self.feature_config['numeric_means'][col])
        return X

# ==================== MODEL LOADING ====================
def load_model_safely(model_path):
    """Load model with proper class definitions"""
    try:
        return joblib.load(model_path)
    except AttributeError as e:
        if "DataSanitizer" in str(e):
            print("‚ö†Ô∏è  Model requires DataSanitizer class. Adding to global scope...")
            import __main__
            __main__.DataSanitizer = DataSanitizer
            return joblib.load(model_path)
        raise

# ==================== UTILITY FUNCTIONS ====================
def normalize_column_names(df: pd.DataFrame):
    """Normalize column names: lowercase, strip spaces, handle common variations"""
    column_mapping = {}
    for col in df.columns:
        normalized = str(col).strip().lower()
        variations = {
            'tenure': ['tenure', 'customer_tenure', 'months_active', 'subscription_length'],
            'numberofaddress': ['numberofaddress', 'address_count', 'num_addresses', 'shipping_addresses'],
            'cashbackamount': ['cashbackamount', 'cashback', 'reward_amount', 'cashback_earned'],
            'daysincelastorder': ['daysincelastorder', 'last_order_days', 'days_since_last', 'recency'],
            'ordercount': ['ordercount', 'total_orders', 'order_count', 'purchase_count'],
            'satisfactionscore': ['satisfactionscore', 'satisfaction', 'customer_score', 'rating']
        }
        for standard_name, aliases in variations.items():
            if normalized in aliases:
                column_mapping[col] = standard_name
                break
        else:
            column_mapping[col] = normalized
    return df.rename(columns=column_mapping)

def find_matching_columns(df: pd.DataFrame, required_columns: List[str]):
    """Find which required columns are present and which are missing"""
    present_columns = [col for col in required_columns if col in df.columns]
    missing_columns = [col for col in required_columns if col not in df.columns]
    return present_columns, missing_columns

# ==================== SUPABASE-BASED DATABASE FUNCTIONS ====================
def save_prediction_to_db(customer_data: CustomerData, prediction: int, probability: float, insights: Optional[List[str]] = None) -> bool:
    """Save prediction results to Supabase"""
    try:
        payload = {
            "tenure": int(customer_data.tenure),
            "numberofaddress": int(customer_data.numberofaddress),
            "cashbackamount": float(customer_data.cashbackamount),
            "daysincelastorder": int(customer_data.daysincelastorder),
            "ordercount": int(customer_data.ordercount),
            "satisfactionscore": int(customer_data.satisfactionscore),
            "churn_prediction": int(prediction),
            "churn_probability": round(float(probability), 3),
            "input_data": customer_data.dict(),       # pass as JSONB
            "insights": insights or [],               # list -> JSONB
        }

        resp = supabase.table("predictions").insert(payload).execute()

        # supabase client returns data in resp.data if success
        if getattr(resp, "data", None):
            return True
        # Some clients return dict with 'data' key
        if isinstance(resp, dict) and resp.get("data"):
            return True

        print("‚ùå Supabase insert unexpected response:", resp)
        return False
    except Exception as e:
        print(f"‚ùå Supabase save error: {e}")
        traceback.print_exc()
        return False

def get_prediction_history(limit: int = 10):
    """Retrieve prediction history from Supabase"""
    try:
        resp = supabase.table("predictions").select("*").order("timestamp", desc=True).limit(limit).execute()
        data = getattr(resp, "data", None) or (resp.get("data") if isinstance(resp, dict) else None)
        if not data:
            return []
        # Normalize timestamp strings where needed
        for rec in data:
            if "timestamp" in rec and rec["timestamp"]:
                # if timestamptz returns as Python datetime string, keep it; else try parsing
                pass
        return data
    except Exception as e:
        print(f"‚ùå Supabase query error: {e}")
        traceback.print_exc()
        return []

# ==================== AI INSIGHT GENERATION ====================
def generate_insights(customer_data, churn_probability):
    try:
        prompt = f"""
        Analyze the following customer data and churn prediction:
        Customer Data: {customer_data}
        Churn Probability: {churn_probability:.2f}

        Provide 3-5 concise and actionable business insights for churn reduction.
        Each insight should be on a separate line, starting with a dash.
        """
        raw_output = ""
        if client:
            try:
                response = client.chat_completion(
                    model="HuggingFaceH4/zephyr-7b-beta",
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=800,
                    temperature=0.7
                )
                raw_output = response.choices[0].message["content"]
            except Exception as chat_error:
                # fallback to text generation
                try:
                    response = client.text_generation(
                        model="google/flan-t5-large",
                        prompt=prompt,
                        max_new_tokens=500,
                        temperature=0.7
                    )
                    raw_output = response.generated_text
                except Exception as tg_error:
                    print("AI generation failed:", chat_error, tg_error)
                    return [f"‚ö†Ô∏è AI Insights unavailable. Error"]
        else:
            return ["‚ö†Ô∏è AI client not configured (HF_API_KEY missing)."]

        insights = [line.lstrip("-‚Ä¢ ").strip() for line in raw_output.split("\n") if line.strip()]
        if len(insights) == 0:
            return [raw_output.strip() or "No insights generated."]
        return insights
    except Exception as e:
        print("generate_insights error:", e)
        traceback.print_exc()
        return [f"‚ö†Ô∏è AI Insights unavailable. Error"]

# ==================== BATCH PROCESSING ====================
def process_batch_from_dataframe(df: pd.DataFrame, filename: str):
    """Process batch prediction from DataFrame efficiently"""
    try:
        if churn_model is None:
            raise HTTPException(status_code=500, detail="Model not loaded")

        print(f"üì® Processing {len(df)} customers from {filename}")

        original_df = df.copy()
        df = normalize_column_names(df)

        required_cols = ['tenure', 'numberofaddress', 'cashbackamount', 'daysincelastorder', 'ordercount', 'satisfactionscore']
        present_cols, missing_cols = find_matching_columns(df, required_cols)
        if missing_cols:
            print(f"‚ö†Ô∏è  Missing columns: {missing_cols}. Using default values.")

        df_clean = df.copy()
        for col in required_cols:
            if col not in df_clean.columns:
                if col in ['tenure', 'numberofaddress', 'daysincelastorder', 'ordercount', 'satisfactionscore']:
                    df_clean[col] = 0
                elif col == 'cashbackamount':
                    df_clean[col] = 0.0
        df_clean = df_clean[required_cols].fillna(0)

        probabilities = churn_model["pipeline"].predict_proba(df_clean)
        prob_churn = probabilities[:, 1]
        threshold = churn_model.get("threshold", 0.17)
        predictions = [1 if p >= threshold else 0 for p in prob_churn]

        results_df = original_df.copy()
        results_df['Churn_Probability'] = prob_churn
        results_df['Churn_Prediction'] = ['Churned' if p >= threshold else 'Retained' for p in predictions]
        results_df['Risk_Level'] = ['high' if p > 0.17 else 'medium' if p > 0.12 else 'low' for p in prob_churn]
        csv_data = results_df.to_csv(index=False)

        results = []
        successful_saves = 0

        for i in range(len(df_clean)):
            row = df_clean.iloc[i]
            customer_data = CustomerData(**{k: row[k] for k in required_cols})
            insights = generate_insights(customer_data, float(prob_churn[i]))
            if missing_cols:
                insights.append(f"üìù Note: Used default values for missing columns: {missing_cols}")

            if save_prediction_to_db(customer_data, predictions[i], float(prob_churn[i]), insights):
                successful_saves += 1

            if i < 100:
                results.append({
                    "row_id": i + 1,
                    "churn_prediction": int(predictions[i]),
                    "churn_probability": round(float(prob_churn[i]), 3),
                    "risk_level": "high" if prob_churn[i] > 0.17 else "medium" if prob_churn[i] > 0.12 else "low",
                    "insights": insights
                })

        print(f"üíæ Successfully saved {successful_saves}/{len(df_clean)} predictions to Supabase")

        churn_count = sum(predictions)
        retention_count = len(predictions) - churn_count
        churn_rate = churn_count / len(predictions) if len(predictions) > 0 else 0
        avg_probability = np.mean(prob_churn) if len(prob_churn) > 0 else 0

        return {
            "filename": filename,
            "total_customers": len(df_clean),
            "saved_to_db": successful_saves,
            "data_quality": {
                "missing_columns": missing_cols,
                "note": "Default values used for missing columns" if missing_cols else "All columns present"
            },
            "predictions": results,
            "csv_data": csv_data,
            "summary": {
                "churn_count": churn_count,
                "retention_count": retention_count,
                "churn_rate": round(float(churn_rate), 3),
                "average_probability": round(float(avg_probability), 3)
            },
            "note": "Only showing first 100 predictions. All data saved to database."
        }

    except Exception as e:
        error_msg = f"Batch processing failed: {str(e)}"
        print(f"‚ùå Error: {error_msg}")
        print(traceback.format_exc())
        raise HTTPException(status_code=500, detail=error_msg)

# ==================== LOAD CHURN MODEL ====================
churn_model = None
model_path = os.getenv("MODEL_PATH", "model/churn_model.pkl")
if os.path.exists(model_path):
    churn_model = load_model_safely(model_path)
else:
    print(f"‚ùå Model file '{model_path}' not found. Please train and export the model as '{model_path}'.")

# ==================== FASTAPI APP ====================
app = FastAPI(title="E-commerce Churn Predictor API", version="1.0.0")
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Serve static files (frontend)
frontend_dir = "frontend"
if os.path.exists(os.path.join(frontend_dir, "css")):
    app.mount("/css", StaticFiles(directory=os.path.join(frontend_dir, "css")), name="css")
if os.path.exists(os.path.join(frontend_dir, "js")):
    app.mount("/js", StaticFiles(directory=os.path.join(frontend_dir, "js")), name="js")
if os.path.exists(os.path.join(frontend_dir, "images")):
    app.mount("/images", StaticFiles(directory=os.path.join(frontend_dir, "images")), name="images")

def create_sample_csv():
    sample_data = """tenure,numberofaddress,cashbackamount,daysincelastorder,ordercount,satisfactionscore
12,2,25.5,15,8,4
24,1,45.2,5,15,5
6,3,12.8,45,3,2
18,2,32.1,8,12,4
36,1,67.8,2,25,5"""
    with open('sample.csv', 'w') as f:
        f.write(sample_data)
    print("‚úÖ Sample CSV created")

if not os.path.exists('sample.csv'):
    create_sample_csv()

# ==================== API ENDPOINTS ====================
@app.get("/", response_class=HTMLResponse)
async def serve_frontend():
    index_path = os.path.join(frontend_dir, "index.html")
    if os.path.exists(index_path):
        with open(index_path, 'r') as f:
            return f.read()
    return """
    <html>
        <head><title>E-commerce Churn Predictor</title></head>
        <body>
            <h1>E-commerce Churn Predictor API</h1>
            <p>Frontend files not found. Please ensure the 'frontend' directory exists.</p>
            <p>API endpoints:</p>
            <ul>
                <li><a href="/health">/health</a> - Health check</li>
                <li><a href="/predictions/history">/predictions/history</a> - Prediction history</li>
                <li><a href="/sample.csv">/sample.csv</a> - Sample CSV file</li>
            </ul>
        </body>
    </html>
    """

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    model_status = "not loaded" if churn_model is None else "loaded"
    return {
        "status": "healthy" if churn_model is not None else "degraded",
        "model_status": model_status,
        "features": churn_model["model_info"]["features_used"] if churn_model else None,
        "database": "Supabase",
        "supabase_url": SUPABASE_URL,
        "timestamp": datetime.utcnow().isoformat()
    }

@app.get("/predictions/history")
async def get_history(limit: int = Query(10, ge=1, le=1000)):
    """Get recent prediction history"""
    try:
        history = get_prediction_history(limit)
        return {"count": len(history), "predictions": history, "limit": limit}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Database error: {str(e)}")

@app.post("/predict", response_model=PredictionResponse)
async def predict_churn(data: CustomerData):
    """Single prediction endpoint"""
    try:
        if churn_model is None:
            raise HTTPException(status_code=500, detail="Model not loaded")

        input_df = pd.DataFrame([{
            'tenure': data.tenure,
            'numberofaddress': data.numberofaddress,
            'cashbackamount': data.cashbackamount,
            'daysincelastorder': data.daysincelastorder,
            'ordercount': data.ordercount,
            'satisfactionscore': data.satisfactionscore
        }])

        probabilities = churn_model["pipeline"].predict_proba(input_df)[0]
        probability = float(probabilities[1])
        threshold = float(churn_model.get("threshold", 0.17))
        prediction = 1 if probability >= threshold else 0

        insights = generate_insights(data, probability)
        saved = save_prediction_to_db(data, prediction, probability, insights)

        return PredictionResponse(
            churn_prediction=int(prediction),
            churn_probability=round(float(probability), 3),
            threshold_used=round(float(threshold), 3),
            risk_level="high" if probability > 0.17 else "medium" if probability > 0.12 else "low",
            insights=insights,
            message="Prediction saved to Supabase!" if saved else "Prediction generated but failed to save."
        )

    except Exception as e:
        error_msg = f"Prediction failed: {str(e)}"
        print(f"‚ùå Error: {error_msg}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=error_msg)

@app.post("/predict/upload/csv")
async def predict_churn_csv(file: UploadFile = File(...)):
    """Process batch predictions from CSV file"""
    try:
        if not file.filename.endswith('.csv'):
            raise HTTPException(status_code=400, detail="File must be a CSV")

        contents = await file.read()
        df = pd.read_csv(io.StringIO(contents.decode('utf-8')))

        result = process_batch_from_dataframe(df, file.filename)
        return result

    except Exception as e:
        error_msg = f"CSV processing failed: {str(e)}"
        print(f"‚ùå Error: {error_msg}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=error_msg)

@app.post("/predict/upload/excel")
async def predict_churn_excel(file: UploadFile = File(...)):
    """Process batch predictions from Excel file"""
    try:
        if not file.filename.endswith(('.xlsx', '.xls')):
            raise HTTPException(status_code=400, detail="File must be Excel format")

        contents = await file.read()
        df = pd.read_excel(io.BytesIO(contents))

        result = process_batch_from_dataframe(df, file.filename)
        return result

    except Exception as e:
        error_msg = f"Excel processing failed: {str(e)}"
        print(f"‚ùå Error: {error_msg}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=error_msg)

@app.post("/predict/upload/csv/download")
async def download_batch_predictions(file: UploadFile = File(...)):
    """Process batch predictions and return full CSV download"""
    try:
        if not file.filename.endswith('.csv'):
            raise HTTPException(status_code=400, detail="File must be a CSV")

        contents = await file.read()
        df = pd.read_csv(io.StringIO(contents.decode('utf-8')))

        result = process_batch_from_dataframe(df, file.filename)

        return Response(
            content=result["csv_data"],
            media_type="text/csv",
            headers={"Content-Disposition": f"attachment; filename=churn_predictions_{file.filename}"}
        )

    except Exception as e:
        print(traceback.format_exc())
        raise HTTPException(status_code=500, detail=f"CSV processing failed: {str(e)}")

@app.get("/sample.csv")
async def get_sample_csv():
    """Serve sample CSV file"""
    return FileResponse("sample.csv", media_type="text/csv", filename="sample.csv")

# OPTIONS handlers for CORS preflight
@app.options("/predict")
async def options_predict():
    return JSONResponse(content={"message": "OK"})

@app.options("/predict/upload/csv")
async def options_predict_csv():
    return JSONResponse(content={"message": "OK"})

@app.options("/predict/upload/excel")
async def options_predict_excel():
    return JSONResponse(content={"message": "OK"})

# ==================== STARTUP EVENT ====================
@app.on_event("startup")
async def startup_event():
    print("üöÄ Starting E-commerce Churn Predictor API...")
    print("üìç API Health: /health")
    print("üìç Sample CSV: /sample.csv")
    print("-" * 50)
    if churn_model:
        try:
            print(f"‚úÖ Model loaded with {len(churn_model['model_info']['features_used'])} features")
        except Exception:
            print("‚úÖ Model loaded")
    else:
        print("‚ùå Model failed to load")

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=int(os.getenv("PORT", 8000)))
